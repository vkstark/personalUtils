#!/usr/bin/env python3
"""
End-to-end test for tool reliability, contracts, and telemetry.

This script tests:
1. ToolExecutor returns ToolExecutionResult
2. ChatEngine collects telemetry
3. Tool metrics are tracked correctly
"""

from ChatSystem.tools.tool_executor import ToolExecutor
from ChatSystem.tools.tool_result import ToolStatus

def test_tool_reliability():
    """Test the complete tool reliability system"""
    print("=" * 60)
    print("Testing Tool Reliability, Contracts & Telemetry")
    print("=" * 60)

    # Setup - use ToolExecutor directly (no need for ChatEngine)
    executor = ToolExecutor()

    print(f"\nâœ“ Initialized ToolExecutor\n")

    # Test 1: Execute a successful tool
    print("Test 1: Execute PathSketch (should succeed)")
    result1 = executor.execute("visualize_directory_tree", {"path": "."})
    print(f"  Status: {result1.status}")
    print(f"  Duration: {result1.duration:.3f}s")
    print(f"  Tool: {result1.tool_name}")
    assert result1.status == ToolStatus.SUCCESS
    print("  âœ“ PASSED\n")

    # Test 2: Execute a tool with error
    print("Test 2: Execute CodeWhisper with invalid path (should error)")
    result2 = executor.execute("analyze_python_code", {"path": "/nonexistent/path"})
    print(f"  Status: {result2.status}")
    print(f"  Error: {result2.error_message}")
    print(f"  Duration: {result2.duration:.3f}s")
    assert result2.status == ToolStatus.ERROR
    print("  âœ“ PASSED\n")

    # Test 3: Check telemetry (simulate through engine)
    print("Test 3: Check tool telemetry collection")

    # Manually record metrics (simulating what ChatEngine does)
    from ChatSystem.core.tool_metrics import ToolMetrics
    from datetime import datetime

    metrics = ToolMetrics(tool_name="visualize_directory_tree")
    metrics.record_execution(result1)

    print(f"  Tool: {metrics.tool_name}")
    print(f"  Total calls: {metrics.total_calls}")
    print(f"  Success count: {metrics.success_count}")
    print(f"  Success rate: {metrics.success_rate:.1f}%")
    print(f"  Avg duration: {metrics.avg_duration:.3f}s")
    print(f"  Health status: {metrics.get_health_status()}")

    assert metrics.total_calls == 1
    assert metrics.success_count == 1
    assert metrics.success_rate == 100.0
    print("  âœ“ PASSED\n")

    # Test 4: Record error and check metrics
    print("Test 4: Record error and verify metrics update")
    metrics2 = ToolMetrics(tool_name="analyze_python_code")
    metrics2.record_execution(result2)

    print(f"  Error count: {metrics2.error_count}")
    print(f"  Last error: {metrics2.last_error}")
    print(f"  Success rate: {metrics2.success_rate:.1f}%")

    assert metrics2.error_count == 1
    assert metrics2.success_rate == 0.0
    print("  âœ“ PASSED\n")

    # Test 5: Verify serialization
    print("Test 5: Verify ToolExecutionResult serialization")
    result_dict = result1.model_dump()
    assert "status" in result_dict
    assert "duration" in result_dict
    assert "tool_name" in result_dict
    print(f"  Serialized fields: {list(result_dict.keys())[:5]}...")
    print("  âœ“ PASSED\n")

    # Test 6: Legacy format conversion
    print("Test 6: Legacy format conversion (backward compatibility)")
    legacy = result1.to_legacy_dict()
    assert "success" in legacy
    assert legacy["success"] is True
    print(f"  Legacy format: {legacy.keys()}")
    print("  âœ“ PASSED\n")

    print("=" * 60)
    print("âœ… ALL TESTS PASSED!")
    print("=" * 60)
    print("\nSummary:")
    print("  â€¢ ToolExecutionResult contract: âœ“")
    print("  â€¢ Success/error handling: âœ“")
    print("  â€¢ Telemetry collection: âœ“")
    print("  â€¢ Metrics tracking: âœ“")
    print("  â€¢ Serialization: âœ“")
    print("  â€¢ Backward compatibility: âœ“")
    print("\nðŸŽ‰ Tool reliability ground layer is operational!\n")

if __name__ == "__main__":
    test_tool_reliability()
